{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly as py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1900.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_float_from_str_decimal(string):\n",
    "    \"\"\" It convert '12,5' to 12.5 \"\"\"\n",
    "    try:\n",
    "        return float(string.replace(',', '.'))\n",
    "    except:\n",
    "        return float(string.replace('.','')[:-3])\n",
    "    \n",
    "to_float_from_str_decimal('1.900,00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "First_GranDataFrame = pd.read_csv('../TFM/First_GranDataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Código municipio</th>\n",
       "      <th>Municipio</th>\n",
       "      <th>2009_1</th>\n",
       "      <th>2009_2</th>\n",
       "      <th>2009_3</th>\n",
       "      <th>2009_4</th>\n",
       "      <th>2010_5</th>\n",
       "      <th>2009_6</th>\n",
       "      <th>2009_7</th>\n",
       "      <th>2009_8</th>\n",
       "      <th>2009_9</th>\n",
       "      <th>2009_10</th>\n",
       "      <th>2009_11</th>\n",
       "      <th>2009_12</th>\n",
       "      <th>2011_13</th>\n",
       "      <th>2009_14</th>\n",
       "      <th>2009_15</th>\n",
       "      <th>2009_16</th>\n",
       "      <th>2009_17</th>\n",
       "      <th>2001_18</th>\n",
       "      <th>2009_19</th>\n",
       "      <th>2009_20</th>\n",
       "      <th>2009_21</th>\n",
       "      <th>2005_22</th>\n",
       "      <th>2005_23</th>\n",
       "      <th>2009_24</th>\n",
       "      <th>2009_25</th>\n",
       "      <th>2005_26</th>\n",
       "      <th>2011_27</th>\n",
       "      <th>2009_28</th>\n",
       "      <th>2009_29</th>\n",
       "      <th>2009_30</th>\n",
       "      <th>2009_31</th>\n",
       "      <th>2009_32</th>\n",
       "      <th>2012</th>\n",
       "      <th>2009_34</th>\n",
       "      <th>2009_35</th>\n",
       "      <th>2009_36</th>\n",
       "      <th>2009_37</th>\n",
       "      <th>2009_38</th>\n",
       "      <th>2009_39</th>\n",
       "      <th>2009_40</th>\n",
       "      <th>2009_41</th>\n",
       "      <th>2009_42</th>\n",
       "      <th>2009_43</th>\n",
       "      <th>2010_44</th>\n",
       "      <th>2010_45</th>\n",
       "      <th>2010_46</th>\n",
       "      <th>2011_47</th>\n",
       "      <th>2009_48</th>\n",
       "      <th>2009_48.1</th>\n",
       "      <th>2011_50</th>\n",
       "      <th>2009_51</th>\n",
       "      <th>2009_52</th>\n",
       "      <th>2009_53</th>\n",
       "      <th>2009_54</th>\n",
       "      <th>2005_55</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006_57</th>\n",
       "      <th>2009_58</th>\n",
       "      <th>2009_59</th>\n",
       "      <th>2009_60</th>\n",
       "      <th>2009_61</th>\n",
       "      <th>2006_62</th>\n",
       "      <th>2007</th>\n",
       "      <th>2009_64</th>\n",
       "      <th>2001_65</th>\n",
       "      <th>2010</th>\n",
       "      <th>2009</th>\n",
       "      <th>2011_partic</th>\n",
       "      <th>estado_animo</th>\n",
       "      <th>satisfaccion_ocio</th>\n",
       "      <th>valor_vida</th>\n",
       "      <th>satisfaccion_vida</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48001</td>\n",
       "      <td>Abadiño</td>\n",
       "      <td>43.75</td>\n",
       "      <td>8,65</td>\n",
       "      <td>0,00</td>\n",
       "      <td>185,36</td>\n",
       "      <td>33,40</td>\n",
       "      <td>0,00</td>\n",
       "      <td>4,55</td>\n",
       "      <td>0,00</td>\n",
       "      <td>2,00</td>\n",
       "      <td>96,44</td>\n",
       "      <td>0,00</td>\n",
       "      <td>6,04</td>\n",
       "      <td>145,52</td>\n",
       "      <td>11,71</td>\n",
       "      <td>7,30</td>\n",
       "      <td>201,11</td>\n",
       "      <td>43,75</td>\n",
       "      <td>88,98</td>\n",
       "      <td>23,68</td>\n",
       "      <td>2,74</td>\n",
       "      <td>2,75</td>\n",
       "      <td>212,71</td>\n",
       "      <td>44,28</td>\n",
       "      <td>100,00</td>\n",
       "      <td>3,41</td>\n",
       "      <td>43,85</td>\n",
       "      <td>73,80</td>\n",
       "      <td>49,69</td>\n",
       "      <td>46,73</td>\n",
       "      <td>13,71</td>\n",
       "      <td>16,76</td>\n",
       "      <td>19,97</td>\n",
       "      <td>4,83</td>\n",
       "      <td>63,27</td>\n",
       "      <td>6,06</td>\n",
       "      <td>13,95</td>\n",
       "      <td>289,99</td>\n",
       "      <td>10,94</td>\n",
       "      <td>289,99</td>\n",
       "      <td>0,73</td>\n",
       "      <td>20,38</td>\n",
       "      <td>9,44</td>\n",
       "      <td>0,00</td>\n",
       "      <td>19,51</td>\n",
       "      <td>56,11</td>\n",
       "      <td>16,89</td>\n",
       "      <td>94,05</td>\n",
       "      <td>4,16</td>\n",
       "      <td>7.260,00</td>\n",
       "      <td>66,02</td>\n",
       "      <td>0,55</td>\n",
       "      <td>185,36</td>\n",
       "      <td>4,55</td>\n",
       "      <td>2,41</td>\n",
       "      <td>1,16</td>\n",
       "      <td>55,01</td>\n",
       "      <td>90,64</td>\n",
       "      <td>8,75</td>\n",
       "      <td>10,40</td>\n",
       "      <td>12,90</td>\n",
       "      <td>6,04</td>\n",
       "      <td>0,88</td>\n",
       "      <td>12,94</td>\n",
       "      <td>5,86</td>\n",
       "      <td>11,24</td>\n",
       "      <td>7,60</td>\n",
       "      <td>8,50</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001</td>\n",
       "      <td>Abaltzisketa</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>269,94</td>\n",
       "      <td>37,60</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>3,00</td>\n",
       "      <td>96,71</td>\n",
       "      <td>0,00</td>\n",
       "      <td>3,12</td>\n",
       "      <td>154,65</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>28,21</td>\n",
       "      <td>25,00</td>\n",
       "      <td>100,00</td>\n",
       "      <td>32,14</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>261,93</td>\n",
       "      <td>49,18</td>\n",
       "      <td>0,00</td>\n",
       "      <td>7,69</td>\n",
       "      <td>52,75</td>\n",
       "      <td>71,60</td>\n",
       "      <td>43,61</td>\n",
       "      <td>69,89</td>\n",
       "      <td>18,35</td>\n",
       "      <td>16,67</td>\n",
       "      <td>4,17</td>\n",
       "      <td>35,00</td>\n",
       "      <td>79,17</td>\n",
       "      <td>15,82</td>\n",
       "      <td>18,67</td>\n",
       "      <td>585,88</td>\n",
       "      <td>0,00</td>\n",
       "      <td>585,88</td>\n",
       "      <td>0,90</td>\n",
       "      <td>12,27</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>14,75</td>\n",
       "      <td>43,53</td>\n",
       "      <td>11,51</td>\n",
       "      <td>100,00</td>\n",
       "      <td>1,90</td>\n",
       "      <td>316,00</td>\n",
       "      <td>74,44</td>\n",
       "      <td>0,00</td>\n",
       "      <td>269,94</td>\n",
       "      <td>9,49</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,18</td>\n",
       "      <td>50,36</td>\n",
       "      <td>110,75</td>\n",
       "      <td>103,45</td>\n",
       "      <td>92,02</td>\n",
       "      <td>18,69</td>\n",
       "      <td>3,12</td>\n",
       "      <td>6,53</td>\n",
       "      <td>10,73</td>\n",
       "      <td>20,61</td>\n",
       "      <td>31,40</td>\n",
       "      <td>22,98</td>\n",
       "      <td>14,93</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48002</td>\n",
       "      <td>Abanto y Ciérvana-Abanto Zierbena</td>\n",
       "      <td>30.77</td>\n",
       "      <td>9,60</td>\n",
       "      <td>0,00</td>\n",
       "      <td>250,51</td>\n",
       "      <td>37,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>2,63</td>\n",
       "      <td>9,77</td>\n",
       "      <td>3,00</td>\n",
       "      <td>98,08</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,41</td>\n",
       "      <td>184,19</td>\n",
       "      <td>9,12</td>\n",
       "      <td>4,56</td>\n",
       "      <td>595,49</td>\n",
       "      <td>30,77</td>\n",
       "      <td>94,70</td>\n",
       "      <td>23,48</td>\n",
       "      <td>3,09</td>\n",
       "      <td>3,11</td>\n",
       "      <td>480,39</td>\n",
       "      <td>92,71</td>\n",
       "      <td>88,17</td>\n",
       "      <td>3,40</td>\n",
       "      <td>42,44</td>\n",
       "      <td>67,20</td>\n",
       "      <td>47,90</td>\n",
       "      <td>51,52</td>\n",
       "      <td>15,68</td>\n",
       "      <td>21,74</td>\n",
       "      <td>13,04</td>\n",
       "      <td>2,39</td>\n",
       "      <td>65,22</td>\n",
       "      <td>6,22</td>\n",
       "      <td>13,99</td>\n",
       "      <td>240,10</td>\n",
       "      <td>7,20</td>\n",
       "      <td>240,10</td>\n",
       "      <td>0,76</td>\n",
       "      <td>4,52</td>\n",
       "      <td>0,00</td>\n",
       "      <td>61,39</td>\n",
       "      <td>20,98</td>\n",
       "      <td>55,96</td>\n",
       "      <td>16,29</td>\n",
       "      <td>84,89</td>\n",
       "      <td>1,63</td>\n",
       "      <td>9.647,00</td>\n",
       "      <td>80,36</td>\n",
       "      <td>1,34</td>\n",
       "      <td>250,51</td>\n",
       "      <td>0,00</td>\n",
       "      <td>10,19</td>\n",
       "      <td>2,16</td>\n",
       "      <td>38,46</td>\n",
       "      <td>76,86</td>\n",
       "      <td>8,60</td>\n",
       "      <td>14,60</td>\n",
       "      <td>9,29</td>\n",
       "      <td>0,41</td>\n",
       "      <td>0,82</td>\n",
       "      <td>8,60</td>\n",
       "      <td>8,15</td>\n",
       "      <td>10,63</td>\n",
       "      <td>21,09</td>\n",
       "      <td>18,13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20002</td>\n",
       "      <td>Aduna</td>\n",
       "      <td>80.00</td>\n",
       "      <td>36,14</td>\n",
       "      <td>0,00</td>\n",
       "      <td>326,34</td>\n",
       "      <td>23,40</td>\n",
       "      <td>46,62</td>\n",
       "      <td>17,99</td>\n",
       "      <td>0,00</td>\n",
       "      <td>1,00</td>\n",
       "      <td>96,71</td>\n",
       "      <td>0,00</td>\n",
       "      <td>-2,41</td>\n",
       "      <td>294,39</td>\n",
       "      <td>2,49</td>\n",
       "      <td>4,99</td>\n",
       "      <td>57,29</td>\n",
       "      <td>80,00</td>\n",
       "      <td>100,00</td>\n",
       "      <td>32,30</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>301,70</td>\n",
       "      <td>39,12</td>\n",
       "      <td>0,00</td>\n",
       "      <td>5,00</td>\n",
       "      <td>52,75</td>\n",
       "      <td>66,10</td>\n",
       "      <td>91,57</td>\n",
       "      <td>55,43</td>\n",
       "      <td>14,46</td>\n",
       "      <td>16,53</td>\n",
       "      <td>26,45</td>\n",
       "      <td>6,56</td>\n",
       "      <td>57,02</td>\n",
       "      <td>17,46</td>\n",
       "      <td>19,20</td>\n",
       "      <td>-900,80</td>\n",
       "      <td>0,00</td>\n",
       "      <td>-900,80</td>\n",
       "      <td>1,79</td>\n",
       "      <td>9,32</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>16,31</td>\n",
       "      <td>53,21</td>\n",
       "      <td>17,38</td>\n",
       "      <td>100,00</td>\n",
       "      <td>2,99</td>\n",
       "      <td>401,00</td>\n",
       "      <td>81,70</td>\n",
       "      <td>2,33</td>\n",
       "      <td>326,34</td>\n",
       "      <td>17,46</td>\n",
       "      <td>4,54</td>\n",
       "      <td>3,71</td>\n",
       "      <td>66,71</td>\n",
       "      <td>108,36</td>\n",
       "      <td>5,03</td>\n",
       "      <td>37,30</td>\n",
       "      <td>12,05</td>\n",
       "      <td>-2,41</td>\n",
       "      <td>1,59</td>\n",
       "      <td>8,34</td>\n",
       "      <td>29,35</td>\n",
       "      <td>14,69</td>\n",
       "      <td>8,29</td>\n",
       "      <td>24,10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20016</td>\n",
       "      <td>Aia</td>\n",
       "      <td>33.33</td>\n",
       "      <td>15,63</td>\n",
       "      <td>0,00</td>\n",
       "      <td>114,60</td>\n",
       "      <td>28,50</td>\n",
       "      <td>0,00</td>\n",
       "      <td>5,03</td>\n",
       "      <td>4,98</td>\n",
       "      <td>3,00</td>\n",
       "      <td>98,90</td>\n",
       "      <td>27,75</td>\n",
       "      <td>4,03</td>\n",
       "      <td>156,28</td>\n",
       "      <td>3,06</td>\n",
       "      <td>3,06</td>\n",
       "      <td>34,96</td>\n",
       "      <td>33,33</td>\n",
       "      <td>98,11</td>\n",
       "      <td>38,45</td>\n",
       "      <td>4,98</td>\n",
       "      <td>5,11</td>\n",
       "      <td>331,52</td>\n",
       "      <td>64,61</td>\n",
       "      <td>83,33</td>\n",
       "      <td>4,82</td>\n",
       "      <td>28,60</td>\n",
       "      <td>73,40</td>\n",
       "      <td>43,87</td>\n",
       "      <td>59,58</td>\n",
       "      <td>15,63</td>\n",
       "      <td>21,78</td>\n",
       "      <td>26,22</td>\n",
       "      <td>32,07</td>\n",
       "      <td>52,00</td>\n",
       "      <td>16,85</td>\n",
       "      <td>17,06</td>\n",
       "      <td>219,79</td>\n",
       "      <td>4,98</td>\n",
       "      <td>219,79</td>\n",
       "      <td>0,72</td>\n",
       "      <td>365,22</td>\n",
       "      <td>0,00</td>\n",
       "      <td>59,41</td>\n",
       "      <td>13,06</td>\n",
       "      <td>51,44</td>\n",
       "      <td>16,77</td>\n",
       "      <td>100,00</td>\n",
       "      <td>4,90</td>\n",
       "      <td>1.958,00</td>\n",
       "      <td>75,42</td>\n",
       "      <td>0,50</td>\n",
       "      <td>114,60</td>\n",
       "      <td>10,73</td>\n",
       "      <td>0,79</td>\n",
       "      <td>0,73</td>\n",
       "      <td>51,96</td>\n",
       "      <td>109,75</td>\n",
       "      <td>4,49</td>\n",
       "      <td>9,97</td>\n",
       "      <td>15,63</td>\n",
       "      <td>4,03</td>\n",
       "      <td>3,98</td>\n",
       "      <td>20,39</td>\n",
       "      <td>17,74</td>\n",
       "      <td>17,67</td>\n",
       "      <td>11,07</td>\n",
       "      <td>14,97</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Código municipio                          Municipio  2009_1 2009_2 2009_3  \\\n",
       "0             48001                            Abadiño   43.75   8,65   0,00   \n",
       "1             20001                       Abaltzisketa   25.00   0,00   0,00   \n",
       "2             48002  Abanto y Ciérvana-Abanto Zierbena   30.77   9,60   0,00   \n",
       "3             20002                              Aduna   80.00  36,14   0,00   \n",
       "4             20016                                Aia   33.33  15,63   0,00   \n",
       "\n",
       "   2009_4 2010_5 2009_6 2009_7 2009_8 2009_9 2009_10 2009_11 2009_12 2011_13  \\\n",
       "0  185,36  33,40   0,00   4,55   0,00   2,00   96,44    0,00    6,04  145,52   \n",
       "1  269,94  37,60   0,00   0,00   0,00   3,00   96,71    0,00    3,12  154,65   \n",
       "2  250,51  37,00   0,00   2,63   9,77   3,00   98,08    0,00    0,41  184,19   \n",
       "3  326,34  23,40  46,62  17,99   0,00   1,00   96,71    0,00   -2,41  294,39   \n",
       "4  114,60  28,50   0,00   5,03   4,98   3,00   98,90   27,75    4,03  156,28   \n",
       "\n",
       "  2009_14 2009_15 2009_16 2009_17 2001_18 2009_19 2009_20 2009_21 2005_22  \\\n",
       "0   11,71    7,30  201,11   43,75   88,98   23,68    2,74    2,75  212,71   \n",
       "1    0,00    0,00   28,21   25,00  100,00   32,14    0,00    0,00  261,93   \n",
       "2    9,12    4,56  595,49   30,77   94,70   23,48    3,09    3,11  480,39   \n",
       "3    2,49    4,99   57,29   80,00  100,00   32,30    0,00    0,00  301,70   \n",
       "4    3,06    3,06   34,96   33,33   98,11   38,45    4,98    5,11  331,52   \n",
       "\n",
       "  2005_23 2009_24 2009_25 2005_26 2011_27 2009_28 2009_29 2009_30 2009_31  \\\n",
       "0   44,28  100,00    3,41   43,85   73,80   49,69   46,73   13,71   16,76   \n",
       "1   49,18    0,00    7,69   52,75   71,60   43,61   69,89   18,35   16,67   \n",
       "2   92,71   88,17    3,40   42,44   67,20   47,90   51,52   15,68   21,74   \n",
       "3   39,12    0,00    5,00   52,75   66,10   91,57   55,43   14,46   16,53   \n",
       "4   64,61   83,33    4,82   28,60   73,40   43,87   59,58   15,63   21,78   \n",
       "\n",
       "  2009_32   2012 2009_34 2009_35 2009_36  2009_37 2009_38  2009_39 2009_40  \\\n",
       "0   19,97   4,83   63,27    6,06   13,95   289,99   10,94   289,99    0,73   \n",
       "1    4,17  35,00   79,17   15,82   18,67   585,88    0,00   585,88    0,90   \n",
       "2   13,04   2,39   65,22    6,22   13,99   240,10    7,20   240,10    0,76   \n",
       "3   26,45   6,56   57,02   17,46   19,20  -900,80    0,00  -900,80    1,79   \n",
       "4   26,22  32,07   52,00   16,85   17,06   219,79    4,98   219,79    0,72   \n",
       "\n",
       "  2009_41 2009_42 2009_43 2010_44 2010_45 2010_46 2011_47 2009_48 2009_48.1  \\\n",
       "0   20,38    9,44    0,00   19,51   56,11   16,89   94,05    4,16  7.260,00   \n",
       "1   12,27    0,00    0,00   14,75   43,53   11,51  100,00    1,90    316,00   \n",
       "2    4,52    0,00   61,39   20,98   55,96   16,29   84,89    1,63  9.647,00   \n",
       "3    9,32    0,00    0,00   16,31   53,21   17,38  100,00    2,99    401,00   \n",
       "4  365,22    0,00   59,41   13,06   51,44   16,77  100,00    4,90  1.958,00   \n",
       "\n",
       "  2011_50 2009_51 2009_52 2009_53 2009_54 2005_55   2005 2006_57 2009_58  \\\n",
       "0   66,02    0,55  185,36    4,55    2,41    1,16  55,01   90,64    8,75   \n",
       "1   74,44    0,00  269,94    9,49    0,26    0,18  50,36  110,75  103,45   \n",
       "2   80,36    1,34  250,51    0,00   10,19    2,16  38,46   76,86    8,60   \n",
       "3   81,70    2,33  326,34   17,46    4,54    3,71  66,71  108,36    5,03   \n",
       "4   75,42    0,50  114,60   10,73    0,79    0,73  51,96  109,75    4,49   \n",
       "\n",
       "  2009_59 2009_60 2009_61 2006_62   2007 2009_64 2001_65   2010   2009  \\\n",
       "0   10,40   12,90    6,04    0,88  12,94    5,86   11,24   7,60   8,50   \n",
       "1   92,02   18,69    3,12    6,53  10,73   20,61   31,40  22,98  14,93   \n",
       "2   14,60    9,29    0,41    0,82   8,60    8,15   10,63  21,09  18,13   \n",
       "3   37,30   12,05   -2,41    1,59   8,34   29,35   14,69   8,29  24,10   \n",
       "4    9,97   15,63    4,03    3,98  20,39   17,74   17,67  11,07  14,97   \n",
       "\n",
       "   2011_partic  estado_animo  satisfaccion_ocio  valor_vida  satisfaccion_vida  \n",
       "0            2             1                  2           2                  1  \n",
       "1            2             2                  1           1                  2  \n",
       "2            2             1                  2           2                  1  \n",
       "3            1             2                  1           1                  2  \n",
       "4            2             2                  1           1                  2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns=None\n",
    "First_GDF = First_GranDataFrame.drop('Unnamed: 0', axis=1)\n",
    "First_GDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "First_GDF_Labels = First_GDF[['2011_partic', 'estado_animo', 'satisfaccion_ocio', 'valor_vida'\n",
    "                             , 'satisfaccion_vida']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_Label_GDF = First_GDF_Labels[['2011_partic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_Label_GDF_estado = First_GDF_Labels[['estado_animo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_Label_GDF_ocio = First_GDF_Labels[['satisfaccion_ocio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_Label_GDF_valorvida = First_GDF_Labels[['valor_vida']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011_partic          int64\n",
       "estado_animo         int64\n",
       "satisfaccion_ocio    int64\n",
       "valor_vida           int64\n",
       "satisfaccion_vida    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "First_GDF_Labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2009_1</th>\n",
       "      <th>2009_2</th>\n",
       "      <th>2009_3</th>\n",
       "      <th>2009_4</th>\n",
       "      <th>2010_5</th>\n",
       "      <th>2009_6</th>\n",
       "      <th>2009_7</th>\n",
       "      <th>2009_8</th>\n",
       "      <th>2009_9</th>\n",
       "      <th>2009_10</th>\n",
       "      <th>2009_11</th>\n",
       "      <th>2009_12</th>\n",
       "      <th>2011_13</th>\n",
       "      <th>2009_14</th>\n",
       "      <th>2009_15</th>\n",
       "      <th>2009_16</th>\n",
       "      <th>2009_17</th>\n",
       "      <th>2001_18</th>\n",
       "      <th>2009_19</th>\n",
       "      <th>2009_20</th>\n",
       "      <th>2009_21</th>\n",
       "      <th>2005_22</th>\n",
       "      <th>2005_23</th>\n",
       "      <th>2009_24</th>\n",
       "      <th>2009_25</th>\n",
       "      <th>2005_26</th>\n",
       "      <th>2011_27</th>\n",
       "      <th>2009_28</th>\n",
       "      <th>2009_29</th>\n",
       "      <th>2009_30</th>\n",
       "      <th>2009_31</th>\n",
       "      <th>2009_32</th>\n",
       "      <th>2012</th>\n",
       "      <th>2009_34</th>\n",
       "      <th>2009_35</th>\n",
       "      <th>2009_36</th>\n",
       "      <th>2009_37</th>\n",
       "      <th>2009_38</th>\n",
       "      <th>2009_39</th>\n",
       "      <th>2009_40</th>\n",
       "      <th>2009_41</th>\n",
       "      <th>2009_42</th>\n",
       "      <th>2009_43</th>\n",
       "      <th>2010_44</th>\n",
       "      <th>2010_45</th>\n",
       "      <th>2010_46</th>\n",
       "      <th>2011_47</th>\n",
       "      <th>2009_48</th>\n",
       "      <th>2009_48.1</th>\n",
       "      <th>2011_50</th>\n",
       "      <th>2009_51</th>\n",
       "      <th>2009_52</th>\n",
       "      <th>2009_53</th>\n",
       "      <th>2009_54</th>\n",
       "      <th>2005_55</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006_57</th>\n",
       "      <th>2009_58</th>\n",
       "      <th>2009_59</th>\n",
       "      <th>2009_60</th>\n",
       "      <th>2009_61</th>\n",
       "      <th>2006_62</th>\n",
       "      <th>2007</th>\n",
       "      <th>2009_64</th>\n",
       "      <th>2001_65</th>\n",
       "      <th>2010</th>\n",
       "      <th>2009</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.75</td>\n",
       "      <td>8.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.36</td>\n",
       "      <td>33.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>96.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.04</td>\n",
       "      <td>145.52</td>\n",
       "      <td>11.71</td>\n",
       "      <td>7.30</td>\n",
       "      <td>201.11</td>\n",
       "      <td>43.75</td>\n",
       "      <td>88.98</td>\n",
       "      <td>23.68</td>\n",
       "      <td>2.74</td>\n",
       "      <td>2.75</td>\n",
       "      <td>212.71</td>\n",
       "      <td>44.28</td>\n",
       "      <td>100.00</td>\n",
       "      <td>3.41</td>\n",
       "      <td>43.85</td>\n",
       "      <td>73.8</td>\n",
       "      <td>49.69</td>\n",
       "      <td>46.73</td>\n",
       "      <td>13.71</td>\n",
       "      <td>16.76</td>\n",
       "      <td>19.97</td>\n",
       "      <td>4.83</td>\n",
       "      <td>63.27</td>\n",
       "      <td>6.06</td>\n",
       "      <td>13.95</td>\n",
       "      <td>289.99</td>\n",
       "      <td>10.94</td>\n",
       "      <td>289.99</td>\n",
       "      <td>0.73</td>\n",
       "      <td>20.38</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.51</td>\n",
       "      <td>56.11</td>\n",
       "      <td>16.89</td>\n",
       "      <td>94.05</td>\n",
       "      <td>4.16</td>\n",
       "      <td>7260.0</td>\n",
       "      <td>66.02</td>\n",
       "      <td>0.55</td>\n",
       "      <td>185.36</td>\n",
       "      <td>4.55</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.16</td>\n",
       "      <td>55.01</td>\n",
       "      <td>90.64</td>\n",
       "      <td>8.75</td>\n",
       "      <td>10.40</td>\n",
       "      <td>12.90</td>\n",
       "      <td>6.04</td>\n",
       "      <td>0.88</td>\n",
       "      <td>12.94</td>\n",
       "      <td>5.86</td>\n",
       "      <td>11.24</td>\n",
       "      <td>7.60</td>\n",
       "      <td>8.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>269.94</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>96.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.12</td>\n",
       "      <td>154.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.21</td>\n",
       "      <td>25.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>32.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>261.93</td>\n",
       "      <td>49.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.69</td>\n",
       "      <td>52.75</td>\n",
       "      <td>71.6</td>\n",
       "      <td>43.61</td>\n",
       "      <td>69.89</td>\n",
       "      <td>18.35</td>\n",
       "      <td>16.67</td>\n",
       "      <td>4.17</td>\n",
       "      <td>35.00</td>\n",
       "      <td>79.17</td>\n",
       "      <td>15.82</td>\n",
       "      <td>18.67</td>\n",
       "      <td>585.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>585.88</td>\n",
       "      <td>0.90</td>\n",
       "      <td>12.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.75</td>\n",
       "      <td>43.53</td>\n",
       "      <td>11.51</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.90</td>\n",
       "      <td>316.0</td>\n",
       "      <td>74.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>269.94</td>\n",
       "      <td>9.49</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.18</td>\n",
       "      <td>50.36</td>\n",
       "      <td>110.75</td>\n",
       "      <td>103.45</td>\n",
       "      <td>92.02</td>\n",
       "      <td>18.69</td>\n",
       "      <td>3.12</td>\n",
       "      <td>6.53</td>\n",
       "      <td>10.73</td>\n",
       "      <td>20.61</td>\n",
       "      <td>31.40</td>\n",
       "      <td>22.98</td>\n",
       "      <td>14.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.77</td>\n",
       "      <td>9.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250.51</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.63</td>\n",
       "      <td>9.77</td>\n",
       "      <td>3.0</td>\n",
       "      <td>98.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.41</td>\n",
       "      <td>184.19</td>\n",
       "      <td>9.12</td>\n",
       "      <td>4.56</td>\n",
       "      <td>595.49</td>\n",
       "      <td>30.77</td>\n",
       "      <td>94.70</td>\n",
       "      <td>23.48</td>\n",
       "      <td>3.09</td>\n",
       "      <td>3.11</td>\n",
       "      <td>480.39</td>\n",
       "      <td>92.71</td>\n",
       "      <td>88.17</td>\n",
       "      <td>3.40</td>\n",
       "      <td>42.44</td>\n",
       "      <td>67.2</td>\n",
       "      <td>47.90</td>\n",
       "      <td>51.52</td>\n",
       "      <td>15.68</td>\n",
       "      <td>21.74</td>\n",
       "      <td>13.04</td>\n",
       "      <td>2.39</td>\n",
       "      <td>65.22</td>\n",
       "      <td>6.22</td>\n",
       "      <td>13.99</td>\n",
       "      <td>240.10</td>\n",
       "      <td>7.20</td>\n",
       "      <td>240.10</td>\n",
       "      <td>0.76</td>\n",
       "      <td>4.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>61.39</td>\n",
       "      <td>20.98</td>\n",
       "      <td>55.96</td>\n",
       "      <td>16.29</td>\n",
       "      <td>84.89</td>\n",
       "      <td>1.63</td>\n",
       "      <td>9647.0</td>\n",
       "      <td>80.36</td>\n",
       "      <td>1.34</td>\n",
       "      <td>250.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.19</td>\n",
       "      <td>2.16</td>\n",
       "      <td>38.46</td>\n",
       "      <td>76.86</td>\n",
       "      <td>8.60</td>\n",
       "      <td>14.60</td>\n",
       "      <td>9.29</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.82</td>\n",
       "      <td>8.60</td>\n",
       "      <td>8.15</td>\n",
       "      <td>10.63</td>\n",
       "      <td>21.09</td>\n",
       "      <td>18.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80.00</td>\n",
       "      <td>36.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>326.34</td>\n",
       "      <td>23.4</td>\n",
       "      <td>46.62</td>\n",
       "      <td>17.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-2.41</td>\n",
       "      <td>294.39</td>\n",
       "      <td>2.49</td>\n",
       "      <td>4.99</td>\n",
       "      <td>57.29</td>\n",
       "      <td>80.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>32.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>301.70</td>\n",
       "      <td>39.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>52.75</td>\n",
       "      <td>66.1</td>\n",
       "      <td>91.57</td>\n",
       "      <td>55.43</td>\n",
       "      <td>14.46</td>\n",
       "      <td>16.53</td>\n",
       "      <td>26.45</td>\n",
       "      <td>6.56</td>\n",
       "      <td>57.02</td>\n",
       "      <td>17.46</td>\n",
       "      <td>19.20</td>\n",
       "      <td>-900.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-900.80</td>\n",
       "      <td>1.79</td>\n",
       "      <td>9.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.31</td>\n",
       "      <td>53.21</td>\n",
       "      <td>17.38</td>\n",
       "      <td>100.00</td>\n",
       "      <td>2.99</td>\n",
       "      <td>401.0</td>\n",
       "      <td>81.70</td>\n",
       "      <td>2.33</td>\n",
       "      <td>326.34</td>\n",
       "      <td>17.46</td>\n",
       "      <td>4.54</td>\n",
       "      <td>3.71</td>\n",
       "      <td>66.71</td>\n",
       "      <td>108.36</td>\n",
       "      <td>5.03</td>\n",
       "      <td>37.30</td>\n",
       "      <td>12.05</td>\n",
       "      <td>-2.41</td>\n",
       "      <td>1.59</td>\n",
       "      <td>8.34</td>\n",
       "      <td>29.35</td>\n",
       "      <td>14.69</td>\n",
       "      <td>8.29</td>\n",
       "      <td>24.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.33</td>\n",
       "      <td>15.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114.60</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.03</td>\n",
       "      <td>4.98</td>\n",
       "      <td>3.0</td>\n",
       "      <td>98.90</td>\n",
       "      <td>27.75</td>\n",
       "      <td>4.03</td>\n",
       "      <td>156.28</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.06</td>\n",
       "      <td>34.96</td>\n",
       "      <td>33.33</td>\n",
       "      <td>98.11</td>\n",
       "      <td>38.45</td>\n",
       "      <td>4.98</td>\n",
       "      <td>5.11</td>\n",
       "      <td>331.52</td>\n",
       "      <td>64.61</td>\n",
       "      <td>83.33</td>\n",
       "      <td>4.82</td>\n",
       "      <td>28.60</td>\n",
       "      <td>73.4</td>\n",
       "      <td>43.87</td>\n",
       "      <td>59.58</td>\n",
       "      <td>15.63</td>\n",
       "      <td>21.78</td>\n",
       "      <td>26.22</td>\n",
       "      <td>32.07</td>\n",
       "      <td>52.00</td>\n",
       "      <td>16.85</td>\n",
       "      <td>17.06</td>\n",
       "      <td>219.79</td>\n",
       "      <td>4.98</td>\n",
       "      <td>219.79</td>\n",
       "      <td>0.72</td>\n",
       "      <td>365.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59.41</td>\n",
       "      <td>13.06</td>\n",
       "      <td>51.44</td>\n",
       "      <td>16.77</td>\n",
       "      <td>100.00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>75.42</td>\n",
       "      <td>0.50</td>\n",
       "      <td>114.60</td>\n",
       "      <td>10.73</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.73</td>\n",
       "      <td>51.96</td>\n",
       "      <td>109.75</td>\n",
       "      <td>4.49</td>\n",
       "      <td>9.97</td>\n",
       "      <td>15.63</td>\n",
       "      <td>4.03</td>\n",
       "      <td>3.98</td>\n",
       "      <td>20.39</td>\n",
       "      <td>17.74</td>\n",
       "      <td>17.67</td>\n",
       "      <td>11.07</td>\n",
       "      <td>14.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.00</td>\n",
       "      <td>8.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>338.98</td>\n",
       "      <td>39.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>98.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.45</td>\n",
       "      <td>350.90</td>\n",
       "      <td>10.46</td>\n",
       "      <td>4.48</td>\n",
       "      <td>102.92</td>\n",
       "      <td>20.00</td>\n",
       "      <td>96.88</td>\n",
       "      <td>12.77</td>\n",
       "      <td>14.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>198.13</td>\n",
       "      <td>24.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.06</td>\n",
       "      <td>28.80</td>\n",
       "      <td>73.4</td>\n",
       "      <td>21.77</td>\n",
       "      <td>47.36</td>\n",
       "      <td>11.81</td>\n",
       "      <td>10.71</td>\n",
       "      <td>23.21</td>\n",
       "      <td>21.33</td>\n",
       "      <td>66.07</td>\n",
       "      <td>7.47</td>\n",
       "      <td>15.99</td>\n",
       "      <td>632.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>632.18</td>\n",
       "      <td>0.80</td>\n",
       "      <td>81.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.05</td>\n",
       "      <td>46.39</td>\n",
       "      <td>12.46</td>\n",
       "      <td>100.00</td>\n",
       "      <td>8.22</td>\n",
       "      <td>669.0</td>\n",
       "      <td>75.94</td>\n",
       "      <td>1.41</td>\n",
       "      <td>338.98</td>\n",
       "      <td>8.97</td>\n",
       "      <td>3.78</td>\n",
       "      <td>4.62</td>\n",
       "      <td>35.08</td>\n",
       "      <td>110.61</td>\n",
       "      <td>4.88</td>\n",
       "      <td>16.95</td>\n",
       "      <td>7.26</td>\n",
       "      <td>1.45</td>\n",
       "      <td>4.53</td>\n",
       "      <td>22.03</td>\n",
       "      <td>22.53</td>\n",
       "      <td>23.08</td>\n",
       "      <td>23.12</td>\n",
       "      <td>40.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>66.67</td>\n",
       "      <td>13.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.44</td>\n",
       "      <td>44.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>88.89</td>\n",
       "      <td>3.0</td>\n",
       "      <td>98.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.23</td>\n",
       "      <td>195.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>60.96</td>\n",
       "      <td>66.67</td>\n",
       "      <td>100.00</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>249.98</td>\n",
       "      <td>33.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.51</td>\n",
       "      <td>72.1</td>\n",
       "      <td>22.32</td>\n",
       "      <td>63.00</td>\n",
       "      <td>20.67</td>\n",
       "      <td>25.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>60.00</td>\n",
       "      <td>6.74</td>\n",
       "      <td>13.71</td>\n",
       "      <td>924.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>924.10</td>\n",
       "      <td>0.55</td>\n",
       "      <td>42.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.36</td>\n",
       "      <td>61.04</td>\n",
       "      <td>27.05</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.57</td>\n",
       "      <td>445.0</td>\n",
       "      <td>75.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64.44</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.21</td>\n",
       "      <td>4.25</td>\n",
       "      <td>54.25</td>\n",
       "      <td>128.19</td>\n",
       "      <td>9.32</td>\n",
       "      <td>33.33</td>\n",
       "      <td>8.93</td>\n",
       "      <td>2.23</td>\n",
       "      <td>10.03</td>\n",
       "      <td>15.05</td>\n",
       "      <td>6.97</td>\n",
       "      <td>12.43</td>\n",
       "      <td>24.00</td>\n",
       "      <td>32.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>66.67</td>\n",
       "      <td>12.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>215.87</td>\n",
       "      <td>40.4</td>\n",
       "      <td>31.75</td>\n",
       "      <td>18.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.74</td>\n",
       "      <td>196.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.19</td>\n",
       "      <td>66.67</td>\n",
       "      <td>98.86</td>\n",
       "      <td>57.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>359.25</td>\n",
       "      <td>63.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.29</td>\n",
       "      <td>52.75</td>\n",
       "      <td>71.8</td>\n",
       "      <td>12.74</td>\n",
       "      <td>59.18</td>\n",
       "      <td>18.59</td>\n",
       "      <td>17.14</td>\n",
       "      <td>22.86</td>\n",
       "      <td>27.91</td>\n",
       "      <td>60.00</td>\n",
       "      <td>12.82</td>\n",
       "      <td>15.06</td>\n",
       "      <td>-412.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-412.77</td>\n",
       "      <td>1.07</td>\n",
       "      <td>34.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.27</td>\n",
       "      <td>49.82</td>\n",
       "      <td>19.64</td>\n",
       "      <td>91.67</td>\n",
       "      <td>1.60</td>\n",
       "      <td>312.0</td>\n",
       "      <td>84.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>215.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.39</td>\n",
       "      <td>64.19</td>\n",
       "      <td>121.94</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.17</td>\n",
       "      <td>19.11</td>\n",
       "      <td>12.74</td>\n",
       "      <td>3.93</td>\n",
       "      <td>6.95</td>\n",
       "      <td>7.96</td>\n",
       "      <td>34.07</td>\n",
       "      <td>22.87</td>\n",
       "      <td>19.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>59.64</td>\n",
       "      <td>23.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>360.57</td>\n",
       "      <td>45.5</td>\n",
       "      <td>11.43</td>\n",
       "      <td>6.10</td>\n",
       "      <td>14.29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.44</td>\n",
       "      <td>200.02</td>\n",
       "      <td>11.54</td>\n",
       "      <td>4.04</td>\n",
       "      <td>222.18</td>\n",
       "      <td>60.61</td>\n",
       "      <td>95.00</td>\n",
       "      <td>59.58</td>\n",
       "      <td>5.71</td>\n",
       "      <td>11.54</td>\n",
       "      <td>301.24</td>\n",
       "      <td>43.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.23</td>\n",
       "      <td>52.75</td>\n",
       "      <td>59.9</td>\n",
       "      <td>33.87</td>\n",
       "      <td>57.69</td>\n",
       "      <td>17.60</td>\n",
       "      <td>22.73</td>\n",
       "      <td>15.91</td>\n",
       "      <td>5.80</td>\n",
       "      <td>61.36</td>\n",
       "      <td>5.77</td>\n",
       "      <td>15.00</td>\n",
       "      <td>134.31</td>\n",
       "      <td>11.43</td>\n",
       "      <td>134.31</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.47</td>\n",
       "      <td>53.51</td>\n",
       "      <td>17.14</td>\n",
       "      <td>100.00</td>\n",
       "      <td>6.35</td>\n",
       "      <td>1733.0</td>\n",
       "      <td>84.16</td>\n",
       "      <td>0.57</td>\n",
       "      <td>360.57</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.13</td>\n",
       "      <td>73.85</td>\n",
       "      <td>88.82</td>\n",
       "      <td>19.96</td>\n",
       "      <td>36.58</td>\n",
       "      <td>16.07</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1.55</td>\n",
       "      <td>5.86</td>\n",
       "      <td>7.37</td>\n",
       "      <td>24.61</td>\n",
       "      <td>25.27</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2009_1  2009_2  2009_3  2009_4  2010_5  2009_6  2009_7  2009_8  2009_9  \\\n",
       "0   43.75    8.65     0.0  185.36    33.4    0.00    4.55    0.00     2.0   \n",
       "1   25.00    0.00     0.0  269.94    37.6    0.00    0.00    0.00     3.0   \n",
       "2   30.77    9.60     0.0  250.51    37.0    0.00    2.63    9.77     3.0   \n",
       "3   80.00   36.14     0.0  326.34    23.4   46.62   17.99    0.00     1.0   \n",
       "4   33.33   15.63     0.0  114.60    28.5    0.00    5.03    4.98     3.0   \n",
       "5   20.00    8.71     0.0  338.98    39.1    0.00   10.46    0.00     3.0   \n",
       "6   66.67   13.39     0.0   64.44    44.1    0.00    0.00   88.89     3.0   \n",
       "7   66.67   12.74     0.0  215.87    40.4   31.75   18.21    0.00     1.0   \n",
       "8   59.64   23.54     0.0  360.57    45.5   11.43    6.10   14.29     1.0   \n",
       "\n",
       "   2009_10  2009_11  2009_12  2011_13  2009_14  2009_15  2009_16  2009_17  \\\n",
       "0    96.44     0.00     6.04   145.52    11.71     7.30   201.11    43.75   \n",
       "1    96.71     0.00     3.12   154.65     0.00     0.00    28.21    25.00   \n",
       "2    98.08     0.00     0.41   184.19     9.12     4.56   595.49    30.77   \n",
       "3    96.71     0.00    -2.41   294.39     2.49     4.99    57.29    80.00   \n",
       "4    98.90    27.75     4.03   156.28     3.06     3.06    34.96    33.33   \n",
       "5    98.90     0.00     1.45   350.90    10.46     4.48   102.92    20.00   \n",
       "6    98.90     0.00     2.23   195.31     0.00     0.00    60.96    66.67   \n",
       "7    96.71     0.00    12.74   196.52     0.00     0.00    24.19    66.67   \n",
       "8    96.71     0.00     3.44   200.02    11.54     4.04   222.18    60.61   \n",
       "\n",
       "   2001_18  2009_19  2009_20  2009_21  2005_22  2005_23  2009_24  2009_25  \\\n",
       "0    88.98    23.68     2.74     2.75   212.71    44.28   100.00     3.41   \n",
       "1   100.00    32.14     0.00     0.00   261.93    49.18     0.00     7.69   \n",
       "2    94.70    23.48     3.09     3.11   480.39    92.71    88.17     3.40   \n",
       "3   100.00    32.30     0.00     0.00   301.70    39.12     0.00     5.00   \n",
       "4    98.11    38.45     4.98     5.11   331.52    64.61    83.33     4.82   \n",
       "5    96.88    12.77    14.12     0.00   198.13    24.91     0.00     6.06   \n",
       "6   100.00     2.94     0.00     0.00   249.98    33.18     0.00     0.00   \n",
       "7    98.86    57.79     0.00     0.00   359.25    63.51     0.00    14.29   \n",
       "8    95.00    59.58     5.71    11.54   301.24    43.29     0.00     3.23   \n",
       "\n",
       "   2005_26  2011_27  2009_28  2009_29  2009_30  2009_31  2009_32   2012  \\\n",
       "0    43.85     73.8    49.69    46.73    13.71    16.76    19.97   4.83   \n",
       "1    52.75     71.6    43.61    69.89    18.35    16.67     4.17  35.00   \n",
       "2    42.44     67.2    47.90    51.52    15.68    21.74    13.04   2.39   \n",
       "3    52.75     66.1    91.57    55.43    14.46    16.53    26.45   6.56   \n",
       "4    28.60     73.4    43.87    59.58    15.63    21.78    26.22  32.07   \n",
       "5    28.80     73.4    21.77    47.36    11.81    10.71    23.21  21.33   \n",
       "6    41.51     72.1    22.32    63.00    20.67    25.00    15.00  33.33   \n",
       "7    52.75     71.8    12.74    59.18    18.59    17.14    22.86  27.91   \n",
       "8    52.75     59.9    33.87    57.69    17.60    22.73    15.91   5.80   \n",
       "\n",
       "   2009_34  2009_35  2009_36  2009_37  2009_38  2009_39  2009_40  2009_41  \\\n",
       "0    63.27     6.06    13.95   289.99    10.94   289.99     0.73    20.38   \n",
       "1    79.17    15.82    18.67   585.88     0.00   585.88     0.90    12.27   \n",
       "2    65.22     6.22    13.99   240.10     7.20   240.10     0.76     4.52   \n",
       "3    57.02    17.46    19.20  -900.80     0.00  -900.80     1.79     9.32   \n",
       "4    52.00    16.85    17.06   219.79     4.98   219.79     0.72   365.22   \n",
       "5    66.07     7.47    15.99   632.18     0.00   632.18     0.80    81.92   \n",
       "6    60.00     6.74    13.71   924.10     0.00   924.10     0.55    42.22   \n",
       "7    60.00    12.82    15.06  -412.77     0.00  -412.77     1.07    34.92   \n",
       "8    61.36     5.77    15.00   134.31    11.43   134.31     0.62     0.00   \n",
       "\n",
       "   2009_42  2009_43  2010_44  2010_45  2010_46  2011_47  2009_48  2009_48.1  \\\n",
       "0     9.44     0.00    19.51    56.11    16.89    94.05     4.16     7260.0   \n",
       "1     0.00     0.00    14.75    43.53    11.51   100.00     1.90      316.0   \n",
       "2     0.00    61.39    20.98    55.96    16.29    84.89     1.63     9647.0   \n",
       "3     0.00     0.00    16.31    53.21    17.38   100.00     2.99      401.0   \n",
       "4     0.00    59.41    13.06    51.44    16.77   100.00     4.90     1958.0   \n",
       "5     0.00     0.00    17.05    46.39    12.46   100.00     8.22      669.0   \n",
       "6     0.00     0.00    18.36    61.04    27.05   100.00     1.57      445.0   \n",
       "7     0.00     0.00    15.27    49.82    19.64    91.67     1.60      312.0   \n",
       "8     0.00     0.00    17.47    53.51    17.14   100.00     6.35     1733.0   \n",
       "\n",
       "   2011_50  2009_51  2009_52  2009_53  2009_54  2005_55   2005  2006_57  \\\n",
       "0    66.02     0.55   185.36     4.55     2.41     1.16  55.01    90.64   \n",
       "1    74.44     0.00   269.94     9.49     0.26     0.18  50.36   110.75   \n",
       "2    80.36     1.34   250.51     0.00    10.19     2.16  38.46    76.86   \n",
       "3    81.70     2.33   326.34    17.46     4.54     3.71  66.71   108.36   \n",
       "4    75.42     0.50   114.60    10.73     0.79     0.73  51.96   109.75   \n",
       "5    75.94     1.41   338.98     8.97     3.78     4.62  35.08   110.61   \n",
       "6    75.12     0.00    64.44     2.25     2.21     4.25  54.25   128.19   \n",
       "7    84.13     0.00   215.87     0.00     0.43     0.39  64.19   121.94   \n",
       "8    84.16     0.57   360.57    -1.15     1.46     0.13  73.85    88.82   \n",
       "\n",
       "   2009_58  2009_59  2009_60  2009_61  2006_62   2007  2009_64  2001_65  \\\n",
       "0     8.75    10.40    12.90     6.04     0.88  12.94     5.86    11.24   \n",
       "1   103.45    92.02    18.69     3.12     6.53  10.73    20.61    31.40   \n",
       "2     8.60    14.60     9.29     0.41     0.82   8.60     8.15    10.63   \n",
       "3     5.03    37.30    12.05    -2.41     1.59   8.34    29.35    14.69   \n",
       "4     4.49     9.97    15.63     4.03     3.98  20.39    17.74    17.67   \n",
       "5     4.88    16.95     7.26     1.45     4.53  22.03    22.53    23.08   \n",
       "6     9.32    33.33     8.93     2.23    10.03  15.05     6.97    12.43   \n",
       "7     1.79     3.17    19.11    12.74     3.93   6.95     7.96    34.07   \n",
       "8    19.96    36.58    16.07     3.44     1.55   5.86     7.37    24.61   \n",
       "\n",
       "    2010   2009  \n",
       "0   7.60   8.50  \n",
       "1  22.98  14.93  \n",
       "2  21.09  18.13  \n",
       "3   8.29  24.10  \n",
       "4  11.07  14.97  \n",
       "5  23.12  40.98  \n",
       "6  24.00  32.79  \n",
       "7  22.87  19.61  \n",
       "8  25.27   7.25  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_GDF = pd.read_csv('../TFM/GranDataframe_predictors.csv',\n",
    "                   converters={'2009_2': to_float_from_str_decimal,\n",
    "                                             '2009_3': to_float_from_str_decimal,\n",
    "                                             '2009_4': to_float_from_str_decimal,\n",
    "                                             '2010_5': to_float_from_str_decimal,\n",
    "                                             '2009_6': to_float_from_str_decimal,\n",
    "                                             '2009_7': to_float_from_str_decimal,\n",
    "                                             '2009_8': to_float_from_str_decimal,\n",
    "                                             '2009_9': to_float_from_str_decimal,\n",
    "                                             '2009_10': to_float_from_str_decimal,\n",
    "                                             '2009_11': to_float_from_str_decimal,\n",
    "                                             '2009_12': to_float_from_str_decimal,\n",
    "                                             '2011_13': to_float_from_str_decimal,\n",
    "                                             '2009_14': to_float_from_str_decimal,\n",
    "                                             '2009_15': to_float_from_str_decimal,\n",
    "                                             '2009_16': to_float_from_str_decimal,\n",
    "                                              '2009_17': to_float_from_str_decimal,\n",
    "                                             '2001_18': to_float_from_str_decimal,\n",
    "                                             '2009_19': to_float_from_str_decimal,\n",
    "                                             '2009_20': to_float_from_str_decimal,\n",
    "                                             '2009_21': to_float_from_str_decimal,\n",
    "                                             '2005_22': to_float_from_str_decimal,\n",
    "                                             '2005_23': to_float_from_str_decimal,\n",
    "                                             '2009_24': to_float_from_str_decimal,\n",
    "                                             '2009_25': to_float_from_str_decimal,\n",
    "                                             '2005_26': to_float_from_str_decimal,\n",
    "                                             '2011_27': to_float_from_str_decimal,\n",
    "                                             '2009_28': to_float_from_str_decimal,\n",
    "                                             '2009_29': to_float_from_str_decimal,\n",
    "                                             '2009_30': to_float_from_str_decimal,\n",
    "                                             '2009_31': to_float_from_str_decimal,\n",
    "                                             '2009_32': to_float_from_str_decimal,\n",
    "                                             '2012': to_float_from_str_decimal,\n",
    "                                             '2009_34': to_float_from_str_decimal,\n",
    "                                             '2009_35': to_float_from_str_decimal,\n",
    "                                             '2009_36': to_float_from_str_decimal,\n",
    "                                             '2009_37': to_float_from_str_decimal,\n",
    "                                             '2009_38': to_float_from_str_decimal,\n",
    "                                             '2009_39': to_float_from_str_decimal,\n",
    "                                             '2009_40': to_float_from_str_decimal,\n",
    "                                             '2009_41': to_float_from_str_decimal,\n",
    "                                             '2009_42': to_float_from_str_decimal,\n",
    "                                             '2009_43': to_float_from_str_decimal,\n",
    "                                             '2010_44': to_float_from_str_decimal,\n",
    "                                             '2010_45': to_float_from_str_decimal,\n",
    "                                             '2010_46': to_float_from_str_decimal,\n",
    "                                             '2011_47': to_float_from_str_decimal,\n",
    "                                             '2009_48': to_float_from_str_decimal,\n",
    "                                             '2009_48.1': to_float_from_str_decimal,\n",
    "                                             '2011_50': to_float_from_str_decimal,\n",
    "                                             '2009_51': to_float_from_str_decimal,\n",
    "                                             '2009_52': to_float_from_str_decimal,\n",
    "                                             '2009_53': to_float_from_str_decimal,\n",
    "                                             '2009_54': to_float_from_str_decimal,\n",
    "                                             '2005_55': to_float_from_str_decimal,\n",
    "                                             '2005': to_float_from_str_decimal,\n",
    "                                             '2006_57': to_float_from_str_decimal,\n",
    "                                             '2009_58': to_float_from_str_decimal,\n",
    "                                             '2009_59': to_float_from_str_decimal,\n",
    "                                             '2009_60': to_float_from_str_decimal,\n",
    "                                             '2009_61': to_float_from_str_decimal,\n",
    "                                             '2006_62': to_float_from_str_decimal,\n",
    "                                             '2007': to_float_from_str_decimal,\n",
    "                                             '2009_64': to_float_from_str_decimal,\n",
    "                                             '2001_65': to_float_from_str_decimal,\n",
    "                                             '2010': to_float_from_str_decimal,\n",
    "                                             '2009': to_float_from_str_decimal,})\n",
    "F_GDF_1 = F_GDF.drop('Unnamed: 0', axis=1)\n",
    "F_GDF_2 = F_GDF_1.drop('Municipio', axis=1)\n",
    "F_GDF_3 = F_GDF_2.drop('Código municipio', axis=1)\n",
    "First_GDF_Predictors = F_GDF_3\n",
    "First_GDF_Predictors.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparando los dos dataset, uno con las variables/predictores y otro con la etiqueta sobre Participacion en las \n",
    "#elecciones municipales (Participativo vs NO participativo)\n",
    "X = First_GDF_Predictors.values\n",
    "y = Unique_Label_GDF.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y) == len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(245, 67)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(245, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = None)\n",
    " \n",
    "# Prepare cross-validation (cv)\n",
    "cv = KFold(n_splits = 5, random_state = None)\n",
    "\n",
    "# Rendimiento\n",
    "p_score = lambda model, score: print('Performance of the %s model is %0.2f%%' % (model, score * 100))\n",
    " \n",
    "#Clasificadores\n",
    "names = [\n",
    "    \"Logistic Regression\", \"Logistic Regression with Polynomial Hypotheses\",\n",
    "    \"Linear SVM\", \"RBF SVM\", \"Neural Net\",\n",
    "]\n",
    " \n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    make_pipeline(PolynomialFeatures(3), LogisticRegression()),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vant/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterando sobre los clasificadores\n",
    "models = []\n",
    "trained_classifiers = []\n",
    "for name, clf in zip(names, classifiers):\n",
    "    scores = []\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        clf.fit(X_train, y_train.ravel())\n",
    "        scores.append( clf.score(X_test, y_test) )\n",
    "    \n",
    "    min_score = min(scores)\n",
    "    max_score = max(scores)\n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    \n",
    "    trained_classifiers.append(clf)\n",
    "    models.append((name, min_score, max_score, avg_score))\n",
    "    \n",
    "fin_models = pd.DataFrame(models, columns = ['Name', 'Min Score', 'Max Score', 'Mean Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Min Score</th>\n",
       "      <th>Max Score</th>\n",
       "      <th>Mean Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RBF SVM</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.408163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression with Polynomial Hypotheses</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.755102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural Net</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.791837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.816327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.836735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Name  Min Score  Max Score  \\\n",
       "3                                         RBF SVM   0.408163   0.408163   \n",
       "1  Logistic Regression with Polynomial Hypotheses   0.755102   0.755102   \n",
       "4                                      Neural Net   0.775510   0.816327   \n",
       "2                                      Linear SVM   0.816327   0.816327   \n",
       "0                             Logistic Regression   0.836735   0.836735   \n",
       "\n",
       "   Mean Score  \n",
       "3    0.408163  \n",
       "1    0.755102  \n",
       "4    0.791837  \n",
       "2    0.816327  \n",
       "0    0.836735  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_models.sort_values(['Mean Score']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "Logistic_Regression_poly_hipot = trained_classifiers[1]\n",
    " \n",
    "data_path = os.path.join(os.getcwd(), \"best-elections.pkl\")\n",
    "pickle.dump(Logistic_Regression_poly_hipot, open(data_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparando los dos dataset, uno con las variables/predictores y otro con la etiqueta sobre estado de animo \n",
    "X = First_GDF_Predictors.values\n",
    "y = Unique_Label_GDF_estado.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = None)\n",
    " \n",
    "# Prepare cross-validation (cv)\n",
    "cv = KFold(n_splits = 5, random_state = None)\n",
    "\n",
    "# Rendimiento\n",
    "p_score = lambda model, score: print('Performance of the %s model is %0.2f%%' % (model, score * 100))\n",
    " \n",
    "#Clasificadores\n",
    "names = [\n",
    "    \"Logistic Regression\", \"Logistic Regression with Polynomial Hypotheses\",\n",
    "    \"Linear SVM\", \"RBF SVM\", \"Neural Net\",\n",
    "]\n",
    " \n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    make_pipeline(PolynomialFeatures(3), LogisticRegression()),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vant/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "trained_classifiers = []\n",
    "for name, clf in zip(names, classifiers):\n",
    "    scores = []\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        clf.fit(X_train, y_train.ravel())\n",
    "        scores.append( clf.score(X_test, y_test) )\n",
    "    \n",
    "    min_score = min(scores)\n",
    "    max_score = max(scores)\n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    \n",
    "    trained_classifiers.append(clf)\n",
    "    models.append((name, min_score, max_score, avg_score))\n",
    "    \n",
    "fin_models = pd.DataFrame(models, columns = ['Name', 'Min Score', 'Max Score', 'Mean Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Min Score</th>\n",
       "      <th>Max Score</th>\n",
       "      <th>Mean Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RBF SVM</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.591837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression with Polynomial Hypotheses</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.877551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.877551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural Net</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.877551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Name  Min Score  Max Score  \\\n",
       "3                                         RBF SVM   0.591837   0.591837   \n",
       "1  Logistic Regression with Polynomial Hypotheses   0.857143   0.857143   \n",
       "0                             Logistic Regression   0.877551   0.877551   \n",
       "2                                      Linear SVM   0.877551   0.877551   \n",
       "4                                      Neural Net   0.836735   0.918367   \n",
       "\n",
       "   Mean Score  \n",
       "3    0.591837  \n",
       "1    0.857143  \n",
       "0    0.877551  \n",
       "2    0.877551  \n",
       "4    0.877551  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_models.sort_values(['Mean Score']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparando los dos dataset, uno con las variables/predictores y otro con la etiqueta sobre satisfaccion con su ocio\n",
    "X = First_GDF_Predictors.values\n",
    "y = Unique_Label_GDF_ocio.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Min Score</th>\n",
       "      <th>Max Score</th>\n",
       "      <th>Mean Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RBF SVM</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.673469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.775510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression with Polynomial Hypotheses</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.795918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.836735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural Net</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.844898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Name  Min Score  Max Score  \\\n",
       "3                                         RBF SVM   0.673469   0.673469   \n",
       "2                                      Linear SVM   0.775510   0.775510   \n",
       "1  Logistic Regression with Polynomial Hypotheses   0.795918   0.795918   \n",
       "0                             Logistic Regression   0.836735   0.836735   \n",
       "4                                      Neural Net   0.836735   0.857143   \n",
       "\n",
       "   Mean Score  \n",
       "3    0.673469  \n",
       "2    0.775510  \n",
       "1    0.795918  \n",
       "0    0.836735  \n",
       "4    0.844898  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_models.sort_values(['Mean Score']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparando los dos dataset, uno con las variables/predictores y otro con la etiqueta sobre valor de la vida\n",
    "X = First_GDF_Predictors.values\n",
    "y = Unique_Label_GDF_valorvida.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Min Score</th>\n",
       "      <th>Max Score</th>\n",
       "      <th>Mean Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RBF SVM</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.734694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression with Polynomial Hypotheses</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.795918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.816327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.816327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural Net</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.844898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Name  Min Score  Max Score  \\\n",
       "3                                         RBF SVM   0.734694   0.734694   \n",
       "1  Logistic Regression with Polynomial Hypotheses   0.795918   0.795918   \n",
       "0                             Logistic Regression   0.816327   0.816327   \n",
       "2                                      Linear SVM   0.816327   0.816327   \n",
       "4                                      Neural Net   0.836735   0.857143   \n",
       "\n",
       "   Mean Score  \n",
       "3    0.734694  \n",
       "1    0.795918  \n",
       "0    0.816327  \n",
       "2    0.816327  \n",
       "4    0.844898  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_models.sort_values(['Mean Score']).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
